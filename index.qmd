---
title: "Syntesis of calibration plots"
author: "Munoz, Debray & de Jong <br>May 1st, 2023"

format: 
  revealjs:
    slide-number: true
    logo: images/logo.png
    css: images/logo.css
    theme: default
    preview-links: auto
    chalkboard: 
      boardmarker-width: 5
from: markdown+emoji
execute:
  echo: false
editor: visual
---

## Today's presentation

<br/>

::: incremental
1.  What is a calibration plot?
2.  Issues with multiple source information
3.  Methods
4.  Simulation
:::

## Prediction model

::: columns
::: {.column width="60%"}
::: fragment
$$y \sim \beta_0+ \beta_1X$$ $$ ln\left(\frac{\pi}{1-\pi}\right)=\beta_0+ \beta_1X$$
:::

::: fragment
[Calibration:]{style="color:#007FFF"}
:::

::: fragment
Agreement between the prediction risk '$\pi$' and the observed proportion of the outcome '$y$'
:::
:::

::: {.column width="40%"}
::: fragment
![Model prediction output (Calibration in large)](images/mod_pred.png){fig-align="right"}
:::
:::
:::

## Calibration plot

::: columns
::: {.column width="30%"}
Group $\pi$ in bins ![](images/bin_prop.png){fig-align="left"}
:::

::: {.column width="70%"}
```{r fig.width=6, fig.height=6}
df <- data.frame(pi=seq(0.1,0.9,0.1),
                 n = c(2,12,10,10,10,15,14,16,11),
                 ny =c(0,2,3,2,5,11,8,12,11))
df$prop<-df$ny/df$n
library(ggplot2)
ggplot(df,aes(x=pi,y=prop))+
  geom_point(color="blue", size=2)+
  geom_line(color="blue")+
  geom_abline(linetype="dashed")+
  theme_light()+
  xlab(expression("Mean predicted value"~(pi)))+ylab("Observed proportion (y=1)")
  
```
:::
:::

:::

## Multiple sources of information

::: columns
::: {.column width="30%"}
![](images/mclust.png){width="80%"}
:::

::: {.column width="70%"}
::: fragment
-   Multiple imputation:

    Cluster = completed data set
:::

::: fragment
-   Individual patient data (IPD):

    Cluster = study data set
:::
:::
:::

```{r}
#| echo: false
library(tidyverse)
library(binom)
df <- data.frame(cluster = rep(c("1","2","K"),each=9),
                 pi = rep(seq(0.1,0.9,0.1),3),
                 n = c(20,120,100,100,100,150,140,160,110),
                 ny =c(0,10,30,20,50,110,80,120,110,1,30,30,NA,NA,NA,NA,NA,NA,NA,NA,NA,80,130,140,137,158,110))

dall<-as_tibble(df)%>%
  mutate(prob=ny/n)%>%
  mutate(n=sample(seq(30,60,by=1), 27, replace = TRUE))%>%
  mutate(y=map2(n,prob,~rbinom(n=.x,size=1,prob=.y)))%>%
  unnest(cols = c(y))%>%
  filter(complete.cases(.))
  
dsum <- dall%>%
         nest( data = -c(cluster,pi))%>%
         mutate(obs_sum = map(data,~.x%>%dplyr::summarize(n_obs = n(),
                                                          n_ev = sum(y,na.rm=T))))%>%
         mutate(test=map(obs_sum,function(x){
                testres<-prop.test(x = x$n_ev, n = x$n_obs, correct = TRUE)
                data.frame(prop=testres$estimate[[1]],lower=testres$conf[1],upper=testres$conf[2])
         }))%>%
         select(-c(data,obs_sum))%>%
         unnest(col=c(test))
```

## Overall calibration curve

::: columns
::: {.column width="30%"}
![](images/mclust.png){width="80%"}
:::

::: {.column width="70%"}
::: fragment
```{r fig.width=6, fig.height=6}
ggplot(dsum, aes(pi, prop, color = cluster)) +
  geom_errorbar(
    aes(ymin = lower, ymax = upper),
    position = position_dodge(0.02), width = 0.01)+
  geom_point( position = position_dodge(0.02))+
  geom_line( position = position_dodge(0.02))+
  scale_color_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  scale_fill_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  xlab(expression("Mean predicted value"~(pi)))+ylab("Observed proportion (y=1)")+
  scale_x_continuous(breaks=seq(0.1,0.9,0.1))+
  geom_abline(linetype="dashed")+theme_light()+
  theme(legend.position = "bottom")

```
:::
:::
:::

:::

## Overall calibration curve

::: columns
::: {.column width="30%"}
![](images/mclust.png){width="80%"}
:::

::: {.column width="70%"}
```{r fig.width=6, fig.height=6}
ggplot(dall, aes(pi, y)) +
  geom_smooth(aes(color = cluster, fill = cluster),method = glm, method.args= list(family="binomial"))+
  scale_x_continuous(breaks=seq(0.1,0.9,0.1))+
  geom_abline(linetype="dashed")+theme_light()+
  scale_color_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  scale_fill_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  xlab(expression("Mean predicted value"~(pi)))+ylab("Observed proportion (y=1)")+
  scale_x_continuous(breaks=seq(0.1,0.9,0.1))+
  geom_abline(linetype="dashed")+theme_light()+
  theme(legend.position = "bottom")
  
```
:::
:::

::: ::: :::

## Overall calibration curve (Average)

::: columns
::: {.column width="30%"}
![](images/mclust.png){width="80%"}
:::

::: {.column width="70%"}
```{r fig.width=6, fig.height=6}
ggplot(dall, aes(pi, y)) +
  geom_smooth(aes(color = cluster, fill = cluster),method = glm, method.args= list(family="binomial"))+
  scale_x_continuous(breaks=seq(0.1,0.9,0.1))+
  geom_abline(linetype="dashed")+theme_light()+
  scale_color_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  scale_fill_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  theme(legend.position = "top")+
  geom_smooth(method = glm, method.args= list(family="binomial"),se=F,color="Purple",linetype="dotted")+
  xlab(expression("Mean predicted value"~(pi)))+ylab("Observed proportion (y=1)")+
  scale_x_continuous(breaks=seq(0.1,0.9,0.1))+
  geom_abline(linetype="dashed")+theme_light()+
  theme(legend.position = "bottom")
  
```
:::
:::

::: ::: :::

## Overall calibration curve (SE)

::: columns
::: {.column width="30%"}
![](images/mclust.png){width="80%"}
:::

::: {.column width="70%"}
```{r fig.width=6, fig.height=6}
ggplot(dall, aes(pi, y)) +
  geom_smooth(aes(color = cluster, fill = cluster),method = glm, method.args= list(family="binomial"))+
  scale_x_continuous(breaks=seq(0.1,0.9,0.1))+
  geom_abline(linetype="dashed")+theme_light()+
  scale_color_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  scale_fill_manual(values=c("#99CCFF", "#99CC99","#FFCC66"))+
  geom_smooth(method = glm, method.args= list(family="binomial"),se=T,color="Purple",linetype="dotted")+
  theme(legend.position = "bottom")
  
```
:::
:::

::: :::

## Methods

-   Stacked
-   Agregated by ID
-   Model based
-   Bootstrapp

## Stacked

![](images/stack.png){width="150%"}

## Agregated by ID

![](images/modid.png){width="150%"}

## Model based 1-step

![](images/mod1s.png){width="150%,height=150%"}

## Model based 2-step (Multivariate)

![](images/mod2m.png){width="120%,height=120%"}

## Model based 2-step (Point-wise)

![](images/mod2p.png){width="150%"}

## Boot-strap

![](images/boot.png){width="150%"}

## Simulation

::: fragment
[Aim:]{style="color:#007FFF"}
:::

::: fragment
Evaluate the performance of these methods
:::

::: fragment
[Data generation:]{style="color:#007FFF"}
:::

::: fragment
-   Multiple imputation
:::

::: fragment
-   IPD ($\pi$ range variation)
:::

## Simulation

::: fragment
[Performance measures:]{style="color:#007FFF"}
:::

::: fragment
[1. Marginal metrics:]{style="color:#99CCFF"}

\- ICI (bias)

\- e50, e90, emax

\- Coverage
:::

::: fragment
[2. Conditional metrics $f(\pi)$:]{style="color:#99CCFF"}

\- Bias

\- Length of the confidence interval

\- Coverage
:::
